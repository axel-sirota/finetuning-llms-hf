{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bx4HBy0PhvSU",
   "metadata": {
    "id": "bx4HBy0PhvSU"
   },
   "source": [
    "# Parameter Efficient Fine Tuning with LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pJt-aQkNuE9E",
   "metadata": {
    "id": "pJt-aQkNuE9E"
   },
   "source": [
    "In this lab we will implement LoRA to finetune DistillBert to perform good Question Anwering with Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fG2m2kt-pQ",
   "metadata": {
    "id": "87fG2m2kt-pQ"
   },
   "source": [
    "## Implementing LoRA on DistillBERT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JXxlI3Xqtx48",
   "metadata": {
    "id": "JXxlI3Xqtx48"
   },
   "source": [
    "Remember that LoRA is a technique where we use 2 low rank matrices to adapt to the output of a given Layer. Like the following image:\n",
    "\n",
    "\n",
    "<img src='https://www.dropbox.com/scl/fi/dfhuc42h5ohcbfny14gg8/lora.png?rlkey=7ku1ocyzibdgmnkup7kmsd8gb&raw=1'  />\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tG3WiuoFAzbX",
   "metadata": {
    "id": "tG3WiuoFAzbX"
   },
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install 'numpy<=1.24'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FB04AvOtpwD_",
   "metadata": {
    "id": "FB04AvOtpwD_"
   },
   "outputs": [],
   "source": [
    "from transformers import TFDistilBertModel, DistilBertConfig, DistilBertTokenizer\n",
    "from transformers.models.distilbert.modeling_tf_distilbert import TFDistilBertMainLayer\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import math\n",
    "import numpy as np\n",
    "import logging\n",
    "import warnings\n",
    "logging.disable(logging.WARNING)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RPdpRfEjt9Zo",
   "metadata": {
    "id": "RPdpRfEjt9Zo"
   },
   "source": [
    "First we implement the Lora Layer like we did before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amS2Bzikp02p",
   "metadata": {
    "id": "amS2Bzikp02p"
   },
   "outputs": [],
   "source": [
    "class LoraLayer(keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        original_layer,\n",
    "        rank=8,\n",
    "        num_heads =1,\n",
    "        dim = 1,\n",
    "        trainable=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # We want to keep the name of this layer the same as the original\n",
    "        # dense layer.\n",
    "        original_layer_config = original_layer.get_config()\n",
    "        name = original_layer_config[\"name\"]\n",
    "\n",
    "        kwargs.pop(\"name\", None)\n",
    "\n",
    "        super().__init__(name=name, trainable=trainable, **kwargs)\n",
    "\n",
    "        self.rank = rank\n",
    "\n",
    "\n",
    "        # Layers.\n",
    "\n",
    "        # Original dense layer.\n",
    "        self.original_layer = original_layer\n",
    "        # No matter whether we are training the model or are in inference mode,\n",
    "        # this layer should be frozen.\n",
    "        self.original_layer.trainable = False\n",
    "\n",
    "        # LoRA dense layers.\n",
    "        self.A = keras.layers.Dense(\n",
    "            units=rank,\n",
    "            use_bias=False,\n",
    "            trainable=trainable,\n",
    "            name=f\"lora_A\",\n",
    "        )\n",
    "\n",
    "        self.B = keras.layers.Dense(\n",
    "            units=dim,\n",
    "            use_bias=False,\n",
    "            trainable=trainable,\n",
    "            name=f\"lora_B\",\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        original_output = self.original_layer(inputs)\n",
    "        if self.trainable:\n",
    "            # If we are fine-tuning the model, we will add LoRA layers' output\n",
    "            # to the original layer's output.\n",
    "            lora_output = self.B(self.A(inputs))\n",
    "            return original_output + lora_output\n",
    "\n",
    "        # If we are in inference mode, we \"merge\" the LoRA layers' weights into\n",
    "        # the original layer's weights - more on this in the text generation\n",
    "        # section!\n",
    "        return original_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Vsn6K26ZvFWR",
   "metadata": {
    "id": "Vsn6K26ZvFWR"
   },
   "source": [
    "Then we iterate and replace each query and value from the MultiHeadAttention layer to LoraLayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TM82NfDoqCG1",
   "metadata": {
    "id": "TM82NfDoqCG1"
   },
   "outputs": [],
   "source": [
    "# Load DistilBert model\n",
    "config = DistilBertConfig()\n",
    "lora_model = TFDistilBertModel(config)\n",
    "\n",
    "# Iterate through the layers and modify the self-attention layers\n",
    "for layer in lora_model.distilbert.transformer.layer:\n",
    "    attention = None # Grab attention layer\n",
    "    dim = None # The dimension of the model\n",
    "    n_heads = None # The number of heads in the model\n",
    "    # Replace query and value weights with LoraLayer instances\n",
    "    attention.q_lin = None # Replace q_lin with a lora layer\n",
    "    attention.v_lin = None # Replace v_lin with a lora layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hj7a27oYqby7",
   "metadata": {
    "id": "hj7a27oYqby7"
   },
   "outputs": [],
   "source": [
    "config = DistilBertConfig()\n",
    "standard_model = TFDistilBertModel(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PaLdl6qCvQUE",
   "metadata": {
    "id": "PaLdl6qCvQUE"
   },
   "source": [
    "We test both implementations work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DGjT0Jr1qzFb",
   "metadata": {
    "id": "DGjT0Jr1qzFb"
   },
   "outputs": [],
   "source": [
    "test_text = [\"This is a test sentence for DistilBert models.\"]\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(test_text, return_tensors='tf', padding=True, truncation=True)\n",
    "\n",
    "# Run the input through the standard model\n",
    "standard_output = standard_model(inputs)\n",
    "# Run the input through the LoRA model\n",
    "lora_output = lora_model(inputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b__R_PCSvTX2",
   "metadata": {
    "id": "b__R_PCSvTX2"
   },
   "source": [
    "Now we set the non-LoraLayers as non-trainable and calculate the trainable weights per layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GHdmCjFB14MX",
   "metadata": {
    "id": "GHdmCjFB14MX"
   },
   "outputs": [],
   "source": [
    "print('Before change: \\n')\n",
    "\n",
    "for layer in lora_model._flatten_layers():\n",
    "    print(layer, layer.__class__.__module__, layer.name, np.sum([np.prod(w.shape) for w in layer.trainable_weights]))\n",
    "\n",
    "    if layer.__class__.__module__.startswith('keras') and not layer.name.startswith(\"lora\"):\n",
    "        layer.trainable = False\n",
    "    elif layer.name.startswith(\"lora\"):\n",
    "        layer.trainable = True\n",
    "    elif layer.name == 'embeddings':\n",
    "        layer.trainable = False\n",
    "\n",
    "print('After change: \\n\\n')\n",
    "\n",
    "for layer in lora_model._flatten_layers():\n",
    "    print(layer, layer.__class__.__module__, layer.name, np.sum([np.prod(w.shape) for w in layer.trainable_weights]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LHtaHBXBq3Mr",
   "metadata": {
    "id": "LHtaHBXBq3Mr"
   },
   "outputs": [],
   "source": [
    "\n",
    "def calculate_trainable_params(model):\n",
    "  return None # Implement the amount of trainable weights\n",
    "\n",
    "print(f\"Trainable parameters in standard DistilBert: {calculate_trainable_params(standard_model)}\")\n",
    "print(f\"Trainable parameters in LoRA-adapted DistilBert: {calculate_trainable_params(lora_model)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f_dE8hwXvd93",
   "metadata": {
    "id": "f_dE8hwXvd93"
   },
   "source": [
    "Notice the difference!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "U0Njbc6fqiC2",
   "metadata": {
    "id": "U0Njbc6fqiC2"
   },
   "outputs": [],
   "source": [
    "standard_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "A31Kf8gxqNaJ",
   "metadata": {
    "id": "A31Kf8gxqNaJ"
   },
   "outputs": [],
   "source": [
    "lora_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LHc5tp2ovg5g",
   "metadata": {
    "id": "LHc5tp2ovg5g"
   },
   "source": [
    "We succesfully Lora-adapted DistillBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aGAy7tBZ5BrV",
   "metadata": {
    "id": "aGAy7tBZ5BrV"
   },
   "source": [
    "## Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ixyJj7dDvqP-",
   "metadata": {
    "id": "ixyJj7dDvqP-"
   },
   "source": [
    "Loading classic stuff to do Q&A plus tokenizing and decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wdor8GVi5CW9",
   "metadata": {
    "id": "wdor8GVi5CW9"
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, TFDistilBertForQuestionAnswering\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def prepare_qa_input(question, context):\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        question,\n",
    "        context,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"tf\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512  # Adjust based on your needs\n",
    "    )\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1VZsNSyd5C1L",
   "metadata": {
    "id": "1VZsNSyd5C1L"
   },
   "outputs": [],
   "source": [
    "model = TFDistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s6GN_iag5FjG",
   "metadata": {
    "id": "s6GN_iag5FjG"
   },
   "outputs": [],
   "source": [
    "question = \"What is the capital of France?\"\n",
    "context = \"France is a country in Europe. Its capital is Paris.\"\n",
    "\n",
    "inputs = prepare_qa_input(question, context)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "\n",
    "start_logits, end_logits = outputs.start_logits, outputs.end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m50zJfsk5LAH",
   "metadata": {
    "id": "m50zJfsk5LAH"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def decode_answer(start_logits, end_logits, inputs):\n",
    "    start_index = tf.argmax(start_logits, axis=1)[0]\n",
    "    end_index = tf.argmax(end_logits, axis=1)[0]\n",
    "\n",
    "    # Convert token indices to the original context text\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    answer_tokens = tokens[start_index: end_index + 1]\n",
    "    answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "\n",
    "    # Check for [CLS] prediction\n",
    "    if answer.startswith(\"[CLS]\"):\n",
    "        return \"Answer not found\"\n",
    "    return answer\n",
    "\n",
    "decoded_answer = decode_answer(start_logits, end_logits, inputs)\n",
    "print(\"Answer:\", decoded_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o1RbUssIvwu-",
   "metadata": {
    "id": "o1RbUssIvwu-"
   },
   "source": [
    "Lora-Adapt this new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eHytxq3d5wBg",
   "metadata": {
    "id": "eHytxq3d5wBg"
   },
   "outputs": [],
   "source": [
    "# Load TFDistilBertForQuestionAnswering model\n",
    "lora_model = TFDistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "\n",
    "# Iterate through the layers and modify the self-attention layers\n",
    "for layer in lora_model.distilbert.transformer.layer:\n",
    "    None # Repeat the same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2UrpUCBy7Vb_",
   "metadata": {
    "id": "2UrpUCBy7Vb_"
   },
   "outputs": [],
   "source": [
    "outputs = lora_model(**inputs)\n",
    "\n",
    "start_logits, end_logits = outputs.start_logits, outputs.end_logits\n",
    "decode_answer(start_logits, end_logits, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WEEIt9bZ7Ntm",
   "metadata": {
    "id": "WEEIt9bZ7Ntm"
   },
   "outputs": [],
   "source": [
    "print('Before change: \\n')\n",
    "\n",
    "for layer in lora_model._flatten_layers():\n",
    "    print(layer, layer.__class__.__module__, layer.name, np.sum([np.prod(w.shape) for w in layer.trainable_weights]))\n",
    "\n",
    "    if layer.__class__.__module__.startswith('keras') and not layer.name.startswith(\"lora\"):\n",
    "        layer.trainable = False\n",
    "    elif layer.name.startswith(\"lora\"):\n",
    "        layer.trainable = True\n",
    "    elif layer.name == 'embeddings':\n",
    "        layer.trainable = False\n",
    "\n",
    "print('After change: \\n\\n')\n",
    "\n",
    "for layer in lora_model._flatten_layers():\n",
    "    print(layer, layer.__class__.__module__, layer.name, np.sum([np.prod(w.shape) for w in layer.trainable_weights]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AIItF-wo7sM1",
   "metadata": {
    "id": "AIItF-wo7sM1"
   },
   "outputs": [],
   "source": [
    "lora_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mm3HeRDlv3PZ",
   "metadata": {
    "id": "mm3HeRDlv3PZ"
   },
   "source": [
    "Now comes the tricky part, we will  use the squad dataset, that has questions and answers, but we need to add the start_positions and end_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZQ1-8nAYLWhM",
   "metadata": {
    "id": "ZQ1-8nAYLWhM"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\", use_fast=True)\n",
    "\n",
    "# Load the dataset (replace with your specific dataset)\n",
    "dataset = None # Load squad validation split\n",
    "\n",
    "def add_token_positions(encodings, answers):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i in range(len(answers)):\n",
    "        start_char = answers[i]['answer_start'][0]\n",
    "        answer_text = answers[i]['text'][0]  # The text of the answer\n",
    "        end_char = start_char + len(answer_text)  # Calculate the end character position\n",
    "\n",
    "        start_idx = encodings.char_to_token(i, start_char)\n",
    "        end_idx = encodings.char_to_token(i, end_char - 1)  # Adjust by -1 for inclusive range\n",
    "\n",
    "        # Set to 0 (default index) if answer is not found within the context\n",
    "        if start_idx is None:\n",
    "            start_idx = 0\n",
    "        if end_idx is None:\n",
    "            end_idx = start_idx  # Default to start index if end index is not found\n",
    "\n",
    "        start_positions.append(start_idx)\n",
    "        end_positions.append(end_idx)\n",
    "\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def add_token_positions_batch(examples):\n",
    "    # Tokenize the examples and then add token positions\n",
    "    tokenized_inputs = tokenizer(examples['question'], examples['context'], truncation=True, padding='max_length', return_offsets_mapping=True, max_length=512)\n",
    "    add_token_positions(tokenized_inputs, examples['answers'])\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Apply the function in a batched manner\n",
    "tokenized_datasets = dataset.map(add_token_positions_batch, batched=True, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KSrqlhOVVf8Z",
   "metadata": {
    "id": "KSrqlhOVVf8Z"
   },
   "outputs": [],
   "source": [
    "train_input_ids = tokenized_datasets['input_ids']\n",
    "train_attention_mask = tokenized_datasets['attention_mask']\n",
    "train_start_positions = tokenized_datasets['start_positions']\n",
    "train_end_positions = tokenized_datasets['end_positions']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Sxl8lTm9XGCb",
   "metadata": {
    "id": "Sxl8lTm9XGCb"
   },
   "outputs": [],
   "source": [
    "train_input_ids = None # Convert to tensor\n",
    "train_attention_mask = None # Convert to tensor\n",
    "train_start_positions = None # Convert to tensor\n",
    "train_end_positions = None # Convert to tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cl7SCsUqwJGl",
   "metadata": {
    "id": "cl7SCsUqwJGl"
   },
   "source": [
    "Testing if everything is OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Zv9CFsbOYP--",
   "metadata": {
    "id": "Zv9CFsbOYP--"
   },
   "outputs": [],
   "source": [
    "# Example: Checking model weights for NaN values\n",
    "for weight in lora_model.weights:\n",
    "    if tf.reduce_any(tf.math.is_nan(weight)):\n",
    "        print(\"NaN weight:\", weight.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5BCMxl9hYbRt",
   "metadata": {
    "id": "5BCMxl9hYbRt"
   },
   "outputs": [],
   "source": [
    "test_output = lora_model(\n",
    "    input_ids=train_input_ids[:1],\n",
    "    attention_mask=train_attention_mask[:1]\n",
    ")\n",
    "if tf.reduce_any(tf.math.is_nan(test_output.start_logits)) or tf.reduce_any(tf.math.is_nan(test_output.end_logits)):\n",
    "    print(\"NaN found in model outputs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GtazWuIFwObI",
   "metadata": {
    "id": "GtazWuIFwObI"
   },
   "source": [
    "Finetuning Lora weights to do Q&A on squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o6SLQflz8kMv",
   "metadata": {
    "id": "o6SLQflz8kMv"
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "lora_model.compile(\n",
    "    optimizer= tf.keras.optimizers.Adam(learning_rate=5e-5, clipnorm=1.0),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9xZPGMs3WxVq",
   "metadata": {
    "id": "9xZPGMs3WxVq"
   },
   "outputs": [],
   "source": [
    "# Create an instance of the callback\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "# Train the model\n",
    "None # Train for 5 epochs and batch size of 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FgkM5CNZ1SiY",
   "metadata": {
    "id": "FgkM5CNZ1SiY"
   },
   "outputs": [],
   "source": [
    "lora_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "araKIGJLzrth",
   "metadata": {
    "id": "araKIGJLzrth"
   },
   "outputs": [],
   "source": [
    "# Your existing code for model prediction\n",
    "question = \"What is the capital of France?\"\n",
    "context = \"France is a country in Europe. Its capital is Paris.\"\n",
    "inputs = prepare_qa_input(question, context)\n",
    "outputs = lora_model(inputs)\n",
    "start_logits, end_logits = outputs.start_logits, outputs.end_logits\n",
    "\n",
    "# Decode the answer\n",
    "decoded_answer = decode_answer(start_logits, end_logits, inputs)\n",
    "print(\"Answer:\", decoded_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GC-7WdhjtO5M",
   "metadata": {
    "id": "GC-7WdhjtO5M"
   },
   "source": [
    "In  case you wanted to create your custom training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obacxZHhajGv",
   "metadata": {
    "id": "obacxZHhajGv"
   },
   "outputs": [],
   "source": [
    "# import tqdm\n",
    "\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=1e-6, epsilon=1e-08, clipnorm=1.0)\n",
    "# loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# # Optionally define metrics, e.g., accuracy\n",
    "# train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "# epochs = 3  # Set the number of epochs\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "#     for step, batch in enumerate(tqdm.tqdm(tf_dataset)):\n",
    "\n",
    "#         # Open a GradientTape\n",
    "#         with tf.GradientTape() as tape:\n",
    "\n",
    "#             # Forward pass\n",
    "#             outputs = lora_model(\n",
    "#                 input_ids=batch['input_ids'],\n",
    "#                 attention_mask=batch['attention_mask']\n",
    "#             )\n",
    "\n",
    "\n",
    "#             # Extract start_logits and end_logits from the model's output\n",
    "#             start_logits = outputs.start_logits\n",
    "#             end_logits = outputs.end_logits\n",
    "\n",
    "#             # Compute the loss value\n",
    "#             start_loss = loss_function(batch['start_positions'], start_logits)\n",
    "\n",
    "#             end_loss = loss_function(batch['end_positions'], end_logits)\n",
    "\n",
    "#             # Compute the total loss\n",
    "#             total_loss = (start_loss + end_loss) / 2\n",
    "\n",
    "#         # Compute gradients\n",
    "#         gradients = tape.gradient(total_loss, lora_model.trainable_variables)\n",
    "#         gradients = [tf.clip_by_value(grad, -1.0, 1.0) for grad in gradients]\n",
    "\n",
    "#         # Update weights\n",
    "#         optimizer.apply_gradients(zip(gradients, lora_model.trainable_variables))\n",
    "\n",
    "#         # Update training metric.\n",
    "#         train_acc_metric.update_state(batch['start_positions'], start_logits)\n",
    "#         train_acc_metric.update_state(batch['end_positions'], end_logits)\n",
    "\n",
    "#         # Log every 200 batches.\n",
    "#         if step % 200 == 0:\n",
    "#             print(\"Training loss (for one batch) at step %d: %.4f\" % (step, float(total_loss)))\n",
    "#             print(\"Seen so far: %s samples\" % ((step + 1) * 16))\n",
    "\n",
    "#     # Display metrics at the end of each epoch.\n",
    "#     train_acc = train_acc_metric.result()\n",
    "#     print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "\n",
    "#     # Reset training metrics at the end of each epoch\n",
    "#     train_acc_metric.reset_states()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p2NRNMPRdBID",
   "metadata": {
    "id": "p2NRNMPRdBID"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}